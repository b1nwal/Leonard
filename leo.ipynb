{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce2d5c5-2c67-46bd-8892-d0d81a511df8",
   "metadata": {},
   "source": [
    "# Leonard\n",
    "Leonard is a functional learning neural network built to learn how to take the coordinate of a point in space and construct a set of rotation angles that map to a kinematic chain to produce an appropriate end effector position.\n",
    "> *Leonard is named for Leonard Hofstadter from the Big Bang Theory, which is a show that I have never watched.*\n",
    "\n",
    "## v1\n",
    "v1 is the experimental phase of leonard. SHort summary of progress can be found on each file\n",
    "\n",
    "## v1.2\n",
    "* Will need to optimize for GPU in ~~v1.2~~\n",
    "    * Project for a v2 build for sure\n",
    "    * Prefetching implemented in v1.2. Potential for GPU speed gains\n",
    "* Sequential API model\n",
    "    * ~~v1.1~~ will change to subclassing Model\n",
    "        * ~~v1.2~~\n",
    "            * ~~Delayed again: still no good reason to do this. Maybe in v2.3~~\n",
    "                * Undelayed: switched to subclassing\n",
    "                    * Changes will continue to be made in code structure and potentially in network structure\n",
    "    * Other suitable changes to architecture will be made\n",
    "* n-straight iterations; no epochs implemented\n",
    "    * > Epochs will not be implemented. \n",
    "    * No tracking metrics\n",
    "        * Some progress has been made here: rolling 100-avg. loss, maxloss on batch\n",
    "* Implement curriculum learning\n",
    "    * v1.2 project, probably.\n",
    "        * In progress\n",
    "* No universal test dataset yet\n",
    "    * Will be used to benchmark ~~all~~ future models\n",
    "        * To be built for ~~v1.1~~\n",
    "            * ~~Don't worry about curriculum learning, dataset will be updated in v1.2 when curriculum is developed~~\n",
    "                * Dataset will be finished for v1.2 before moving on to v1.3 or v2.0\n",
    "            \n",
    "* Rudimentary [Training Loop](https://console.paperspace.com/pablo6400/notebook/rpz97cf0w8bv7ns?file=%2Fleo.v1.ipynb#Training-Loop)\n",
    "* Loss function based on Euclidean Distance; ~~v1.1 will regress each axis individually~~\n",
    "    * Not possible. ~~Switching to MSE~~\n",
    "        * Outcomes were worse\n",
    "* Fixed issue where euclidean loss function was apparently reducing over entire batch. \n",
    "    * Appears to have significantly impacted training outcomes\n",
    "* Switched random sampling on datagenerator to use gaussian distribution due to rotational invariance\n",
    "* Attempted switch to mean squared error\n",
    "    * Outcomes were worse\n",
    "\n",
    "## The plan for v2\n",
    "v2 will be the final incarnation of Leonard. Major differences:\n",
    "* Penalization for long motor movements\n",
    "* Constraints on movement\n",
    "     * Maybe add small noise to coax regeneration to a spot where the problem is solveable?\n",
    "* Measurements fully accurate to model\n",
    "* Hopefully I will make better use of markdown in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2ab102e4-7124-4477-900e-ba770ef76129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.559657Z",
     "iopub.status.busy": "2025-05-21T23:36:24.559066Z",
     "iopub.status.idle": "2025-05-21T23:36:24.563566Z",
     "shell.execute_reply": "2025-05-21T23:36:24.562787Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.559631Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e214860-de7a-4098-ae8f-31278c9f7099",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "## Hyperparameters\n",
    " - Learning rate\n",
    " - Batch size\n",
    " - Train dataset size\n",
    " - (currently unused) dropout rate between central layers\n",
    " \n",
    "## Arm Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "528a3aa6-c487-47b2-9415-ea45575beb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.565704Z",
     "iopub.status.busy": "2025-05-21T23:36:24.565152Z",
     "iopub.status.idle": "2025-05-21T23:36:24.570473Z",
     "shell.execute_reply": "2025-05-21T23:36:24.569627Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.565676Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"train_dataset_size\": 15000,\n",
    "    \"corpus_callosum\": 0.0,\n",
    "    \"replay_max_probability\": 0.8,\n",
    "    \"replay_buffer_length\": 1000,\n",
    "    \"jitter_multiplier\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "528e8055-cb0b-4506-9db5-bb46e9cb30d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.572484Z",
     "iopub.status.busy": "2025-05-21T23:36:24.571734Z",
     "iopub.status.idle": "2025-05-21T23:36:24.576132Z",
     "shell.execute_reply": "2025-05-21T23:36:24.575237Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.572450Z"
    }
   },
   "outputs": [],
   "source": [
    "armparameters = {\n",
    "    \"seglengths\": [1,1,1,1,1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2c9f2-92e6-454c-9843-6e3380b07cef",
   "metadata": {},
   "source": [
    "# $f^{-1}(x)$\n",
    "> The general idea is that  \n",
    "> $f^{-1}(f(x)) = x$, so  \n",
    "> loss = lossfn($f^{-1}(f(x))$, $x$)  \n",
    "> Where $f(x)$ is the function that the neural network is trying to learn.\n",
    "\n",
    "I am calling this ***Inverse Functional Training***, or ***IFT***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "021d781c-10ab-4007-b022-e7e3d1957bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.579272Z",
     "iopub.status.busy": "2025-05-21T23:36:24.578579Z",
     "iopub.status.idle": "2025-05-21T23:36:24.583583Z",
     "shell.execute_reply": "2025-05-21T23:36:24.582684Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.579236Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eudist(x,y):\n",
    "    return tf.norm(y-x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "eb7895d7-5144-4ceb-b940-3e98c53acace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.585890Z",
     "iopub.status.busy": "2025-05-21T23:36:24.585132Z",
     "iopub.status.idle": "2025-05-21T23:36:24.597919Z",
     "shell.execute_reply": "2025-05-21T23:36:24.597248Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.585867Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def build_rotary_matrix(angle):\n",
    "  r1 = tf.stack([tf.cos(angle), 0., tf.sin(angle)])\n",
    "  r2 = tf.constant([0.,1.,0.],dtype=\"float32\")\n",
    "  r3 = tf.stack([-1*tf.sin(angle),0.,tf.cos(angle)])\n",
    "  return tf.stack([r1,r2,r3])\n",
    "@tf.function\n",
    "def build_joint_matrix(angle):\n",
    "  r1 = tf.constant([1., 0., 0.],dtype=\"float32\")\n",
    "  r2 = tf.stack([0.,tf.cos(angle),-1*tf.sin(angle)])\n",
    "  r3 = tf.stack([0.,tf.sin(angle),tf.cos(angle)])\n",
    "  return tf.stack([r1,r2,r3])\n",
    "@tf.function\n",
    "def fwd(rotation_angles): # I am coming back for you: you will be so optimized later dude\n",
    "    t = tf.transpose(rotation_angles)\n",
    "    rotary1_matrices = tf.vectorized_map(build_rotary_matrix, t[0],fallback_to_while_loop=False)\n",
    "    joint1_matrices = tf.matmul(rotary1_matrices, tf.vectorized_map(build_joint_matrix, t[1],fallback_to_while_loop=False))\n",
    "    joint2_matrices = tf.matmul(joint1_matrices, tf.vectorized_map(build_joint_matrix, t[2],fallback_to_while_loop=False))\n",
    "    rotary2_matrices = tf.matmul(joint2_matrices, tf.vectorized_map(build_rotary_matrix, t[3],fallback_to_while_loop=False))\n",
    "    joint3_matrices = tf.matmul(rotary2_matrices, tf.vectorized_map(build_joint_matrix, t[4],fallback_to_while_loop=False))\n",
    "    \n",
    "    segd = tf.constant([[0],[1],[0]],dtype=\"float32\") # TODO change this to match arm measurements\n",
    "    \n",
    "    seg1 = tf.matmul(rotary1_matrices, segd)\n",
    "    seg2 = tf.matmul(joint1_matrices, segd)\n",
    "    seg3 = tf.matmul(joint2_matrices, segd)\n",
    "    seg4 = tf.matmul(rotary2_matrices, segd)\n",
    "    seg5 = tf.matmul(joint3_matrices, segd)\n",
    "    \n",
    "    a = tf.concat([seg1,seg2,seg3,seg4,seg5],axis=2)\n",
    "    b = tf.reduce_sum(a,axis=2)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723ded8-c4bb-44e3-949a-9b97ccbc02f0",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "### coord_datagen() > coord_dataset\n",
    "### replay ---------^\n",
    "Chance of experience replay is proportional to length of replay buffer\n",
    "\n",
    "* Should there be a recency bias?\n",
    "    * How would that be implemented?\n",
    "    * Is it necessary for such a short queue\n",
    "* v1.2 implements prefetching, autotuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0571b1f0-92c1-4cc2-8b95-ec701ebe8f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.600095Z",
     "iopub.status.busy": "2025-05-21T23:36:24.599148Z",
     "iopub.status.idle": "2025-05-21T23:36:24.603164Z",
     "shell.execute_reply": "2025-05-21T23:36:24.602495Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.600070Z"
    }
   },
   "outputs": [],
   "source": [
    "replay = deque(maxlen=hyperparameters[\"replay_buffer_length\"]) # stores inputs in the form [[1,2,3,4,5],[...]...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5294c96e-5558-498d-bfde-52e446d78fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.604459Z",
     "iopub.status.busy": "2025-05-21T23:36:24.604024Z",
     "iopub.status.idle": "2025-05-21T23:36:24.609649Z",
     "shell.execute_reply": "2025-05-21T23:36:24.609108Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.604436Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = hyperparameters[\"batch_size\"]\n",
    "replay_max_prob = hyperparameters[\"replay_max_probability\"]\n",
    "replay_max_len = hyperparameters[\"replay_buffer_length\"]\n",
    "train_dataset_size = hyperparameters[\"train_dataset_size\"]\n",
    "\n",
    "def coord_datagen(): # outputs a batch of coordinates that are approximately uniformly distributed in end effector space.\n",
    "    for i in range(batch_size * train_dataset_size):  \n",
    "        replay_buff_len = len(replay)\n",
    "        if (np.random.uniform() < (replay_buff_len / (replay_max_len * replay_max_prob**-1))):\n",
    "            r = replay.pop()\n",
    "        else:\n",
    "            r = tf.random.uniform(minval=-np.pi, maxval=np.pi, shape=(5,))\n",
    "        yield r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9d872227-5bc6-4dda-9ea0-8583d0f00050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.611154Z",
     "iopub.status.busy": "2025-05-21T23:36:24.610440Z",
     "iopub.status.idle": "2025-05-21T23:36:24.641148Z",
     "shell.execute_reply": "2025-05-21T23:36:24.640501Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.611131Z"
    }
   },
   "outputs": [],
   "source": [
    "coord_dataset = tf.data.Dataset.from_generator(\n",
    "    coord_datagen, \n",
    "    output_signature=tf.TensorSpec((5,),dtype=\"float32\")\n",
    ").batch(hyperparameters[\"batch_size\"]).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352652e-b52c-49e6-a046-1d8e319b5c42",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "leo is defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "59aab56a-164b-44ec-9875-d8861ec601a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.643152Z",
     "iopub.status.busy": "2025-05-21T23:36:24.642326Z",
     "iopub.status.idle": "2025-05-21T23:36:24.751179Z",
     "shell.execute_reply": "2025-05-21T23:36:24.750562Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.643114Z"
    }
   },
   "outputs": [],
   "source": [
    "class Leonard(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.D2 = tf.keras.layers.Dense(192)\n",
    "        self.BN2 = tf.keras.layers.BatchNormalization()\n",
    "        # activation\n",
    "        \n",
    "        self.D3 = tf.keras.layers.Dense(192)\n",
    "        self.BN3 = tf.keras.layers.BatchNormalization()\n",
    "        # activation\n",
    "        \n",
    "        self.D4 = tf.keras.layers.Dense(192)\n",
    "        self.BN4 = tf.keras.layers.BatchNormalization()\n",
    "        # activation\n",
    "        \n",
    "        self.corpus_callosum = tf.keras.layers.Dropout(hyperparameters[\"corpus_callosum\"])\n",
    "        \n",
    "        self.D5 = tf.keras.layers.Dense(192)\n",
    "        self.BN5 = tf.keras.layers.BatchNormalization()\n",
    "        # activation\n",
    "        \n",
    "        self.D6 = tf.keras.layers.Dense(192)\n",
    "        self.BN6 = tf.keras.layers.BatchNormalization()\n",
    "        # activation\n",
    "        \n",
    "        self.D7 = tf.keras.layers.Dense(5,kernel_regularizer=\"l2\",dtype=\"float32\")\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"learning_rate\"])\n",
    "        self.kicked_in = False\n",
    "        # self.collecting = false\n",
    "    def call(self,x,training=False):\n",
    "        # x /= 5\n",
    "        y = tf.keras.activations.swish(self.D2(x))\n",
    "        y = tf.keras.activations.swish(self.D3(y))\n",
    "        y = tf.keras.activations.swish(self.D4(y))\n",
    "        y = self.corpus_callosum(y,training=training)\n",
    "        y = tf.keras.activations.swish(self.D5(y))\n",
    "        y = tf.keras.activations.swish(self.D6(y))\n",
    "        y = self.D7(y)\n",
    "        return y\n",
    "    \n",
    "    @tf.function\n",
    "    def grads(self, g):\n",
    "        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n",
    "        \n",
    "leo = Leonard()\n",
    "leo.build(input_shape=(hyperparameters[\"batch_size\"], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "cce6480e-cb45-4a26-92bf-2b7c9d757c3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.753895Z",
     "iopub.status.busy": "2025-05-21T23:36:24.753479Z",
     "iopub.status.idle": "2025-05-21T23:36:24.844156Z",
     "shell.execute_reply": "2025-05-21T23:36:24.843453Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.753872Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 25 variables. \n"
     ]
    }
   ],
   "source": [
    "leo.load_weights(\"leo_v1-2-3/step_40000.keras\") # start from hard 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ecda9-06c0-446b-aa56-97cd621cf9f0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "6960c999-9efc-409f-9136-e374bbac8ad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:36:24.845871Z",
     "iopub.status.busy": "2025-05-21T23:36:24.845082Z",
     "iopub.status.idle": "2025-05-21T23:39:35.571237Z",
     "shell.execute_reply": "2025-05-21T23:39:35.570171Z",
     "shell.execute_reply.started": "2025-05-21T23:36:24.845832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "[===>                 ] Sample: 2718/15000, Sampl. Loss: 0.0364, Max Loss: 0.1177*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 190.552 s\n",
       "File: /tmp/ipykernel_40/1958280877.py\n",
       "Function: train at line 2\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     2                                           def train():\n",
       "     3                                               \n",
       "     4                                               # qinator = deque(maxlen=1000)\n",
       "     5                                               # losslog = open(\"losslog.txt\",\"a\")\n",
       "     6                                               \n",
       "     7      2720 5406845735.0    2e+06      2.8      for f,x in enumerate(coord_dataset):\n",
       "     8      2720 7218270544.0    3e+06      3.8          fx = fwd(x)\n",
       "     9      5440  109419631.0  20113.9      0.1          with tf.GradientTape() as tape:\n",
       "    10      2720        4e+10    2e+07     23.1              y = leo(fx,training=True)\n",
       "    11      2720        1e+10    4e+06      6.1              loss = eudist(fwd(y),fx)\n",
       "    12      2720        8e+10    3e+07     42.2          gradient = tape.gradient(loss, leo.trainable_variables)\n",
       "    13      2720        1e+10    5e+06      7.0          leo.grads(gradient)\n",
       "    14                                           \n",
       "    15      2720 1266778068.0 465727.2      0.7          mmax = tf.reduce_max(loss)\n",
       "    16      2720 1009713983.0 371218.4      0.5          mloss = tf.reduce_mean(loss)\n",
       "    17                                                   # qinator.append(mloss)\n",
       "    18                                                   # mean = tf.Variable(0., trainable=False)\n",
       "    19                                           \n",
       "    20      2720    5201112.0   1912.2      0.0          if f > 1000:\n",
       "    21      1719        1e+10    7e+06      6.1              v = tf.boolean_mask(x,(loss>1))\n",
       "    22      1718 1787901593.0    1e+06      0.9              if (tf.size(v) > 0):\n",
       "    23        74   29590250.0 399868.2      0.0                  for ex in v:\n",
       "    24        38     106927.0   2813.9      0.0                      replay.append(ex)\n",
       "    25                                           \n",
       "    26      2719   11554891.0   4249.7      0.0          if f%1000==0: # periodic backup save\n",
       "    27         3  548649208.0    2e+08      0.3              leo.save((\"leo_v1-2-3-hard/step_\"+str(f)+\".keras\"))\n",
       "    28      2719   13678764.0   5030.8      0.0          percentage = int((f/hyperparameters[\"train_dataset_size\"])*20)\n",
       "    29      2719    2403262.0    883.9      0.0          h = hyperparameters[\"train_dataset_size\"]\n",
       "    30      2719        1e+10    4e+06      6.4          tf.print(\"\\r[\"+\"=\"*percentage + \">\" + \" \"*(20-percentage) + \"]\",\"Sample: {f}/{h}, Sampl. Loss: {l:.4f}, Max Loss: {mxl:.4f}\".format(f=f,l=mloss,mxl=mmax,h=hyperparameters[\"train_dataset_size\"]),end=\"\")\n",
       "    31                                                   # losslog.write(str(l2loss.numpy()) + \"\\n\")\n",
       "    32                                               # losslog.close()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "def train():\n",
    "    \n",
    "    # qinator = deque(maxlen=1000)\n",
    "    # losslog = open(\"losslog.txt\",\"a\")\n",
    "    \n",
    "    for f,x in enumerate(coord_dataset):\n",
    "        fx = fwd(x)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = leo(fx,training=True)\n",
    "            loss = eudist(fwd(y),fx)\n",
    "        gradient = tape.gradient(loss, leo.trainable_variables)\n",
    "        leo.grads(gradient)\n",
    "\n",
    "        mmax = tf.reduce_max(loss)\n",
    "        mloss = tf.reduce_mean(loss)\n",
    "        # qinator.append(mloss)\n",
    "        # mean = tf.Variable(0., trainable=False)\n",
    "\n",
    "        if f > 1000:\n",
    "            v = tf.boolean_mask(x,(loss>1))\n",
    "            if (tf.size(v) > 0):\n",
    "                for ex in v:\n",
    "                    replay.append(ex)\n",
    "\n",
    "        if f%1000==0: # periodic backup save\n",
    "            leo.save((\"leo_v1-2-3-hard/step_\"+str(f)+\".keras\"))\n",
    "        percentage = int((f/hyperparameters[\"train_dataset_size\"])*20)\n",
    "        h = hyperparameters[\"train_dataset_size\"]\n",
    "        tf.print(\"\\r[\"+\"=\"*percentage + \">\" + \" \"*(20-percentage) + \"]\",\"Sample: {f}/{h}, Sampl. Loss: {l:.4f}, Max Loss: {mxl:.4f}\".format(f=f,l=mloss,mxl=mmax,h=hyperparameters[\"train_dataset_size\"]),end=\"\")\n",
    "        # losslog.write(str(l2loss.numpy()) + \"\\n\")\n",
    "    # losslog.close()\n",
    "    \n",
    "%lprun -f train train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "374b0b5e-ffe0-4cb9-a82b-3e785e641a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:39:35.573035Z",
     "iopub.status.busy": "2025-05-21T23:39:35.572401Z",
     "iopub.status.idle": "2025-05-21T23:39:35.576755Z",
     "shell.execute_reply": "2025-05-21T23:39:35.575880Z",
     "shell.execute_reply.started": "2025-05-21T23:39:35.573001Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch_size=1\n",
    "# leo.load_weights(\"leo_v1-2-3/step_40000.keras\")\n",
    "# # # r = tf.cast(np.random.uniform(size=(batch_size,5)), \"float32\")\n",
    "\n",
    "# inp = tf.constant([[4.,1.,0.]],dtype=\"float32\")\n",
    "# a = leo(inp)\n",
    "# print(fwd(a))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "521289d5-1527-4027-801a-59cc6a0491c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T23:39:35.578584Z",
     "iopub.status.busy": "2025-05-21T23:39:35.577880Z",
     "iopub.status.idle": "2025-05-21T23:39:35.581801Z",
     "shell.execute_reply": "2025-05-21T23:39:35.581182Z",
     "shell.execute_reply.started": "2025-05-21T23:39:35.578539Z"
    }
   },
   "outputs": [],
   "source": [
    "# leo.save(\"leo_conclusory.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
